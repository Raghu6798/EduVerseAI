transcribe videos, and answer questions based on the content.

 ### The below tree structure of the multi-page Web App 
 ```
C:
‚îÇ   app.py
‚îú‚îÄ‚îÄ‚îÄpages
‚îÇ       1_Document_QA.py
‚îÇ       2_Image_QA.py
‚îÇ       3_Video_QA.py is a streamlit project ,  Document_QA.py has import streamlit as st 
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.vectorstores import FAISS
from langchain_community.docstore import InMemoryDocstore
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_cerebras import ChatCerebras
from langchain_mistralai import ChatMistralAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.document_loaders import DirectoryLoader
from streamlit_pdf_viewer import pdf_viewer
from langchain.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq
from langchain.schema import StrOutputParser
from uuid import uuid4
import faiss
import os
from dotenv import load_dotenv
import logging
import asyncio

load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

if 'pdf_ref' not in st.session_state:
    st.session_state.pdf_ref = None

# Async function to invoke chain
async def async_invoke_chain(chain, input_data):
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, chain.invoke, input_data)

# Initialize session state for messages and models
if "messages" not in st.session_state:
    st.session_state["messages"] = []


llm = ChatGroq(
    model="deepseek-r1-distill-llama-70b ",
    temperature=0.8,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    api_key=os.getenv("GROQ_API_KEY"),
)



if "models" not in st.session_state:
    st.session_state["models"] = {
        "Gemini": ChatGoogleGenerativeAI(
            model="gemini-2.0-flash",
            temperature=0.8,
            verbose=True,
            api_key=os.getenv("GOOGLE_AI_STUDIO_API_KEY")
        ),
        "Deepseek-R1-distill-llama-70b": ChatGroq(
            model="deepseek-r1-distill-llama-70b",
            temperature=0.8,
            max_tokens=None,
            timeout=None,
            max_retries=2,
            api_key=os.getenv("GROQ_API_KEY"),
        ),
        "Mistral": ChatMistralAI(
            model_name="open-mistral-nemo",
            temperature=0.8,
            verbose=True
        ),
        "Llama": ChatCerebras(
            model="llama-3.3-70b",
            temperature=0.8,
            verbose=True,
            api_key=os.getenv("CEREBRAS_API_KEY")
        )
    }

if "embeddings" not in st.session_state:
    model_name = "sentence-transformers/all-mpnet-base-v2"
    model_kwargs = {'device': 'cpu'}
    encode_kwargs = {'normalize_embeddings': True}
    st.session_state["embeddings"] = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )

# Streamlit UI Header
st.header("üóÇÔ∏è ‚ú® Document Question Answering")
st.write("""Upload a document and query its content. Supported formats include:
- PDF Files (.pdf)
- Word Documents (.docx)
""")

# File uploader for document
uploaded_doc = st.file_uploader("Upload your document (.pdf, .docx):", type=["pdf", "docx"])

# Process uploaded PDF document
if uploaded_doc and uploaded_doc.name.endswith(".pdf"):
    # Store the uploaded PDF file in session state for preview in sidebar
    st.session_state.pdf_ref = uploaded_doc

    # Display PDF preview in the sidebar
    with st.sidebar:
        binary_data = st.session_state.pdf_ref.getvalue()
        pdf_viewer(input=binary_data, width=700)

    # Process the PDF file
    with st.spinner("Processing the uploaded PDF document..."):
        # Save the uploaded file temporarily
        temp_path = f"temp_{uuid4().hex}.pdf"
        with open(temp_path, "wb") as f:
            f.write(uploaded_doc.read())

        # Load document using PyMuPDFLoader
        loader = PyMuPDFLoader(temp_path)
        documents = loader.load()

        # Remove the temporary file
        os.remove(temp_path)

        st.success(f"Successfully loaded {len(documents)} pages from the uploaded PDF.")

        # Embed the documents into FAISS index
        text_splitter = RecursiveCharacterTextSplitter(separators=["\n\n"], chunk_size=1200, chunk_overlap=200)
        chunks = text_splitter.split_documents(documents)
        embedding_dim = len(st.session_state["embeddings"].embed_query("hello world"))
        index = faiss.IndexFlatL2(embedding_dim)
        vector_store = FAISS(
            embedding_function=st.session_state["embeddings"],
            index=index,
            docstore=InMemoryDocstore(),
            index_to_docstore_id={},
        )
        ids = [str(uuid4()) for _ in range(len(chunks))]
        vector_store.add_documents(chunks, ids=ids)

        for idx, doc_id in enumerate(ids):
            vector_store.index_to_docstore_id[idx] = doc_id

        # Create retriever with the FAISS index
        doc_retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 3})

        def get_retrieved_context(query):
            retrieved_documents = doc_retriever.get_relevant_documents(query)
            return "\n".join(doc.page_content for doc in retrieved_documents)

        # User input for querying the document
        user_input = st.chat_input("Ask your queries about the document/documents:")

        # Define prompt template
        prompt_template = ChatPromptTemplate.from_messages([
    ("system", """
        You are an expert document analyst with the ability to process large volumes of text efficiently. 
        Your task is to extract key insights and answer questions based on the content of the provided document : {context}
        When asked a question, you should provide a direct, detailed, and concise response, only using the information available from the document. 
        If the answer cannot be found directly, you should clarify this and provide relevant context or related information if applicable.
        Focus on uncovering critical information, whether it's specific facts, summaries, or hidden insights within the document.
    """),
    ("human", "{question}")
])


        # Handle user input and display responses
        if user_input:
            st.session_state["messages"].append({"role": "user", "content": user_input})
            qa_chain = prompt_template | st.session_state["models"]["Deepseek-R1-distill-llama-70b":] | StrOutputParser()
            context = get_retrieved_context(user_input)
            response_message = asyncio.run(async_invoke_chain(qa_chain, {"question": user_input, "context": context}))
            st.session_state["messages"].append({"role": "assistant", "content": response_message})

            for message in st.session_state["messages"]:
                st.chat_message(message["role"]).markdown(message["content"])   , Image_QA.py has import streamlit as st
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_community.docstore import InMemoryDocstore
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.messages import HumanMessage
from langchain_cerebras import ChatCerebras
from langchain_mistralai import ChatMistralAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from langchain_groq import ChatGroq
from uuid import uuid4
import faiss
import os
from dotenv import load_dotenv
import logging
import httpx
import base64
import asyncio

# Initialize environment variables and logging
load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Async function to invoke chain
async def async_invoke_chain(chain, input_data):
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, chain.invoke, input_data)

# Initialize session state for messages and models
if "messages" not in st.session_state:
    st.session_state.messages = []
if "models" not in st.session_state:
    st.session_state["models"] = {
        "Gemini": ChatGoogleGenerativeAI(
            model="gemini-2.0-flash",
            temperature=0.8,
            verbose=True,
            api_key=os.getenv("GOOGLE_AI_STUDIO_API_KEY")
        ),
        "Deepseek-R1-distill-llama-70b": ChatGroq(
            model="deepseek-r1-distill-llama-70b",
            temperature=0.8,
            max_tokens=None,
            timeout=None,
            max_retries=2,
            api_key=os.getenv("GROQ_API_KEY"),
        ),
        "Mistral": ChatMistralAI(
            model_name="open-mistral-nemo",
            temperature=0.8,
            verbose=True
        ),
        "Llama": ChatCerebras(
            model="llama-3.3-70b",
            temperature=0.8,
            verbose=True,
            api_key=os.getenv("CEREBRAS_API_KEY")
        )
    }

# Initialize embeddings model
if "embeddings" not in st.session_state:
    model_name = "sentence-transformers/all-mpnet-base-v2"
    model_kwargs = {'device': 'cpu'}
    encode_kwargs = {'normalize_embeddings': False}
    st.session_state.embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )

st.header("üì∏üìàüìä ÷é Image Content Analysis and Question Answering")

# Brief overview for image content analysis
description = """
Upload an image, and the AI will analyze its content and answer your questions. 
It can interpret various types of images including:
- General imagery (objects, people, scenes)
- Diagrams, graphs, and data visualizations
- Scientific and medical images
- Text-based images (documents, screenshots)
"""

# Display the brief description
st.write(description)

# File upload and URL input
st.header("Upload Image for Question Answering")
uploaded_file = st.file_uploader("Upload an image (.jpeg, .jpg, .png, etc.):", type=["jpeg", "jpg", "png"])

st.header("Or Enter the Image URL :")
image_url = st.text_input("Enter the image URL")

image_data = None

if uploaded_file:
    st.image(uploaded_file, caption="Uploaded Image", use_container_width=True)
    image_data = base64.b64encode(uploaded_file.read()).decode("utf-8")
elif image_url:
    try:
        with httpx.Client() as client:
            response = client.get(image_url)
            response.raise_for_status()
            st.image(response.content, caption="Image from URL", use_container_width=True)
            image_data = base64.b64encode(response.content).decode("utf-8")
    except Exception as e:
        st.error(f"Error fetching image from URL: {e}")

if image_data:
    message = HumanMessage(content=[{
            "type": "text", "text": "Describe what is in the image in detail."
        }, {
            "type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}
        }])

    # Generate response from the model
    response = asyncio.run(async_invoke_chain(st.session_state.models["Gemini"], [message]))
    knowledge = [Document(page_content=response.content)]

    # Split text into chunks for indexing
    text_splitter = RecursiveCharacterTextSplitter(separators="\n\n", chunk_size=1500, chunk_overlap=200)
    chunks = text_splitter.split_documents(knowledge)

    # Create FAISS IndexHNSWFlat for indexing image embeddings
    index = faiss.IndexFlatL2(len(st.session_state.embeddings.embed_query("hello world")))

    # Create FAISS vector store for document retrieval
    vector_store = FAISS(
        embedding_function=st.session_state.embeddings,
        index=index,
        docstore=InMemoryDocstore(),
        index_to_docstore_id={},
    )

    # Generate unique IDs and add documents to the store
    ids = [str(uuid4()) for _ in range(len(chunks))]
    vector_store.add_documents(documents=chunks, ids=ids)

    # Update the mapping between FAISS index and document IDs
    for idx, doc_id in enumerate(ids):
        vector_store.index_to_docstore_id[idx] = doc_id

    # Create image retriever with the FAISS index
    image_retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 6})

    def get_retrieved_context(query):
        retrieved_documents = image_retriever.get_relevant_documents(query)
        return "\n".join(doc.page_content for doc in retrieved_documents)

    # User query for image QA
    user_input = st.chat_input("Ask a question about the image:")

    prompt = ChatPromptTemplate.from_messages([(
            "system", "You are an expert image analyst trained to detect and explain the differences between real and AI-generated images. Your analysis must be thorough, highlighting patterns, textures, and anomalies unique to each category.This is the data regarding the image {context} , use it to answer the query."
        ), ("human", "{question}")])

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        qa_chain = prompt | st.session_state.models["Deepseek-R1-distill-llama-70b"] | StrOutputParser()
        context = get_retrieved_context(user_input)
        response_message = asyncio.run(async_invoke_chain(qa_chain, {"question": user_input, "context": context}))
        st.session_state.messages.append({"role": "assistant", "content": response_message})
        for message in st.session_state.messages:
            st.chat_message(message["role"]).markdown(message["content"]) and Video_QA.py has import streamlit as st
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import YoutubeLoader
from langchain_community.docstore import InMemoryDocstore
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_cerebras import ChatCerebras
from langchain_mistralai import ChatMistralAI
from langchain_groq import ChatGroq
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from uuid import uuid4
import whisper
import torch
import tempfile
import faiss
from dotenv import load_dotenv
import logging
import asyncio
import os

# Load environment variables
load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize session state for messages
if "messages" not in st.session_state:
    st.session_state.messages = []

async def async_invoke_chain(chain, input_data):
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, chain.invoke, input_data)

if "models" not in st.session_state:
    st.session_state["models"] = {
        "Gemini": ChatGoogleGenerativeAI(
            model="gemini-2.0-flash",
            temperature=0.8,
            verbose=True,
            api_key=os.getenv("GOOGLE_AI_STUDIO_API_KEY")
        ),
        "Deepseek-R1-distill-llama-70b": ChatGroq(
            model="deepseek-r1-distill-llama-70b",
            temperature=0.8,
            max_tokens=None,
            timeout=None,
            max_retries=2,
            api_key=os.getenv("GROQ_API_KEY"),
        ),
        "Mistral": ChatMistralAI(
            model_name="open-mistral-nemo",
            temperature=0.8,
            verbose=True
        ),
        "Llama": ChatCerebras(
            model="llama-3.3-70b",
            temperature=0.8,
            verbose=True,
            api_key=os.getenv("CEREBRAS_API_KEY")
        )
    }


# Initialize embeddings
if "embeddings" not in st.session_state:
    model_name = "sentence-transformers/all-mpnet-base-v2"
    device = "cuda" if torch.cuda.is_available() else "cpu"
    st.session_state.embeddings = HuggingFaceEmbeddings(
        model_name=model_name, model_kwargs={"device": device}, encode_kwargs={"normalize_embeddings": False}
    )

# Recursive text splitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=200)

# Prompt template
prompt = ChatPromptTemplate.from_messages([
    ("system", """You are an expert explainer of video content. Your goal is to provide comprehensive and insightful answers to user questions based on the provided video transcript. You will combine information from the transcript with your general knowledge to give a well-rounded understanding.

Here's how you should approach each question:

1. **Video Introductory Overview:** First, directly answer the user's question using relevant excerpts from the provided transcript. Use quotation marks to clearly indicate text taken directly from the transcript.

2. **Detailed Explanation:** Expand on the transcript's information with detailed explanations, context, and background information from your general knowledge. Explain any technical terms or concepts that might be unfamiliar to the user.

3. **Examples and Analogies:** Use examples, analogies, and real-world scenarios to illustrate complex ideas and make them easier to understand.

4. **Code Snippets/URLs (If Applicable):** If the video discusses code or refers to external resources, provide relevant code snippets (formatted for readability) or URLs to further enhance the explanation.

5. **Structure and Clarity:** Present your answers in a clear, structured, and easy-to-read format. Use headings, bullet points, and numbered lists where appropriate.

Context (Video Transcript):
{context}"""),
    ("user", "{question}")
])

st.title("Video QA with LangChain ü¶úüîó & Streamlit")

# Upload video file
uploaded_video = st.file_uploader("Upload a video file", type=["mp4", "mov", "avi"])

video_url = None

if uploaded_video:
    st.video(uploaded_video)
    if st.button("Generate Transcript from Video"):
        with st.spinner("Transcribing video..."):
            try:
                # Save the uploaded video file to a temporary file
                with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp_file:
                    temp_file.write(uploaded_video.getvalue())
                    temp_file_path = temp_file.name

                # Load Whisper model and transcribe the video
                model = whisper.load_model("small")
                model = model.to("cpu")
                result = model.transcribe(temp_file_path)

                # Get the transcript text
                transcript = result["text"]
                docs = [Document(page_content=transcript)]
                chunks = text_splitter.split_documents(docs)

                # **Clear the previous vector store**
                index = faiss.IndexFlatL2(len(st.session_state.embeddings.embed_query("hello world")))
                st.session_state.vector_store = FAISS(
                    embedding_function=st.session_state.embeddings,
                    index=index,
                    docstore=InMemoryDocstore(),
                    index_to_docstore_id={},
                )

                # Add the new documents to the vector store
                ids = [str(uuid4()) for _ in range(len(chunks))]
                st.session_state.vector_store.add_documents(documents=chunks, ids=ids)
                st.success("You are ready to Ask anything about the Video")

            except Exception as e:
                st.error(f"Error fetching transcript: {e}")

else:
    # YouTube video input
    video_url = st.text_input("Enter YouTube video URL:")

    if video_url and st.button("Generate Transcript from YouTube"):
        st.video(video_url)
        
        with st.spinner("Fetching transcript..."):
            try:
                # Load transcript using YoutubeLoader
                loader = YoutubeLoader.from_youtube_url(
                    video_url, add_video_info=False
                )
                transcript = loader.load()

                # Split into documents for chunking
                docs = [Document(page_content=entry.page_content) for entry in transcript]
                chunks = text_splitter.split_documents(docs)

                # **Clear the previous vector store**
                index = faiss.IndexFlatL2(len(st.session_state.embeddings.embed_query("hello world")))
                st.session_state.vector_store = FAISS(
                    embedding_function=st.session_state.embeddings,
                    index=index,
                    docstore=InMemoryDocstore(),
                    index_to_docstore_id={},
                )

                # Add the new documents to the vector store
                ids = [str(uuid4()) for _ in range(len(chunks))]
                st.session_state.vector_store.add_documents(documents=chunks, ids=ids)
                st.success("You are ready to Ask anything about the Video")

            except Exception as e:
                st.error(f"Error fetching transcript: {e}")

# QA Section
if "vector_store" in st.session_state:
    def get_retrieved_context(query):
        video_retriever = st.session_state.vector_store.as_retriever(
            search_type="similarity", search_kwargs={"k": 2}
        )
        
        retrieved_documents = video_retriever.get_relevant_documents(query)
        return "\n".join(doc.page_content for doc in retrieved_documents)

    user_input = st.chat_input("Ask a question about the video:")
    if user_input:
        context = get_retrieved_context(user_input)
        qa_chain = prompt | st.session_state.models[ "Deepseek-R1-distill-llama-70b"] | StrOutputParser()
        response_message = asyncio.run(async_invoke_chain(qa_chain, {"question": user_input, "context": context}))

        st.session_state.messages.append({"role": "user", "content": user_input})
        st.session_state.messages.append({"role": "assistant", "content": response_message})

        for message in st.session_state.messages:
            st.chat_message(message["role"]).markdown(message["content"])
else:
    st.error("No transcription available. Please upload or process a video first.") create production ready fastAPI routes for the above code individually into 